{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'emails/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFiles = []\n",
    "# for root, folder, files in os.walk(path):\n",
    "#     for file in files:\n",
    "#         dataFiles.append(root+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r'E:\\data\\emails\\emails\\ham\\00002.9c4069e25e1ef370c078db7ee85ff9ac')\n",
    "data = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = False\n",
    "d = []\n",
    "for line in data:\n",
    "    if line == \"\\n\":\n",
    "        body = True\n",
    "    if body:\n",
    "        d.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'Martin A posted:\\n',\n",
       " 'Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\\n',\n",
       " ' limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\\n',\n",
       " ' Mount Athos monastic community, was ideal for the patriotic sculpture. \\n',\n",
       " ' \\n',\n",
       " \" As well as Alexander's granite features, 240 ft high and 170 ft wide, a\\n\",\n",
       " ' museum, a restored amphitheatre and car park for admiring crowds are\\n',\n",
       " 'planned\\n',\n",
       " '---------------------\\n',\n",
       " 'So is this mountain limestone or granite?\\n',\n",
       " \"If it's limestone, it'll weather pretty fast.\\n\",\n",
       " '\\n',\n",
       " '------------------------ Yahoo! Groups Sponsor ---------------------~-->\\n',\n",
       " '4 DVDs Free +s&p Join Now\\n',\n",
       " 'http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\\n',\n",
       " '---------------------------------------------------------------------~->\\n',\n",
       " '\\n',\n",
       " 'To unsubscribe from this group, send an email to:\\n',\n",
       " 'forteana-unsubscribe@egroups.com\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " 'Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    for root, folder, files in os.walk(path):\n",
    "        for file in files:\n",
    "            filename = root+'/'+file\n",
    "            file = io.open(filename,encoding='latin1')\n",
    "            body = False\n",
    "            data = []\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if line == '\\n':\n",
    "                    body = True\n",
    "                if body:\n",
    "                    data.append(line)\n",
    "            file.close()\n",
    "            msg = '\\n'.join(data)\n",
    "            yield msg, filename\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neeraj Kumar\\AppData\\Local\\Temp\\ipykernel_22264\\1960606276.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append(df('E:\\data\\emails\\emails\\ham', 'ham'))\n",
      "C:\\Users\\Neeraj Kumar\\AppData\\Local\\Temp\\ipykernel_22264\\1960606276.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append(df('E:\\data\\emails\\emails\\spam', 'spam'))\n"
     ]
    }
   ],
   "source": [
    "def df(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for message, filename in read_file(path):\n",
    "        rows.append({'message':message,'class':classification})\n",
    "        index.append(filename)\n",
    "        \n",
    "    return pd.DataFrame(data=rows, index=index)\n",
    "\n",
    "dataset = pd.DataFrame({'message':[], 'class':[]})\n",
    "dataset = dataset.append(df('E:\\data\\emails\\emails\\ham', 'ham'))\n",
    "dataset = dataset.append(df('E:\\data\\emails\\emails\\spam', 'spam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E:\\data\\emails\\emails\\ham/00001.7c53336b37003a9286aba55d2945844c</th>\n",
       "      <td>\\n\\n    Date:        Wed, 21 Aug 2002 10:54:46...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E:\\data\\emails\\emails\\ham/00002.9c4069e25e1ef370c078db7ee85ff9ac</th>\n",
       "      <td>\\n\\nMartin A posted:\\n\\nTassos Papadopoulos, t...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E:\\data\\emails\\emails\\ham/00003.860e3c3cee1b42ead714c5c874fe25f7</th>\n",
       "      <td>\\n\\nMan Threatens Explosion In Moscow \\n\\n\\n\\n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E:\\data\\emails\\emails\\ham/00004.864220c5b6930b209cc287c361c99af1</th>\n",
       "      <td>\\n\\nKlez: The Virus That Won't Die\\n\\n \\n\\nAlr...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E:\\data\\emails\\emails\\ham/00005.bf27cdeaf0b8c4647ecd61b1d09da613</th>\n",
       "      <td>\\n\\n&gt;  in adding cream to spaghetti carbonara,...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              message  \\\n",
       "E:\\data\\emails\\emails\\ham/00001.7c53336b37003a9...  \\n\\n    Date:        Wed, 21 Aug 2002 10:54:46...   \n",
       "E:\\data\\emails\\emails\\ham/00002.9c4069e25e1ef37...  \\n\\nMartin A posted:\\n\\nTassos Papadopoulos, t...   \n",
       "E:\\data\\emails\\emails\\ham/00003.860e3c3cee1b42e...  \\n\\nMan Threatens Explosion In Moscow \\n\\n\\n\\n...   \n",
       "E:\\data\\emails\\emails\\ham/00004.864220c5b6930b2...  \\n\\nKlez: The Virus That Won't Die\\n\\n \\n\\nAlr...   \n",
       "E:\\data\\emails\\emails\\ham/00005.bf27cdeaf0b8c46...  \\n\\n>  in adding cream to spaghetti carbonara,...   \n",
       "\n",
       "                                                   class  \n",
       "E:\\data\\emails\\emails\\ham/00001.7c53336b37003a9...   ham  \n",
       "E:\\data\\emails\\emails\\ham/00002.9c4069e25e1ef37...   ham  \n",
       "E:\\data\\emails\\emails\\ham/00003.860e3c3cee1b42e...   ham  \n",
       "E:\\data\\emails\\emails\\ham/00004.864220c5b6930b2...   ham  \n",
       "E:\\data\\emails\\emails\\ham/00005.bf27cdeaf0b8c46...   ham  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, punkt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = dataset['message'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = re.findall('[\\w]+',msg)\n",
    "tokens = []\n",
    "for i in range(len(dataset)):\n",
    "    msg = re.findall('[\\w]+',dataset['message'][i])\n",
    "    tokens.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(df):\n",
    "    tokens = []\n",
    "    for i in range(len(df)):\n",
    "#         tokens.append(word_tokenize(df['message'][i]))\n",
    "        msg = re.findall('[\\w]+',df['message'][i])\n",
    "        tokens.append(msg)\n",
    "\n",
    "    stopwordsList = stopwords.words(\"english\")\n",
    "#     stopwordsList.extend([',','.','-','!',':','>','<','\\n','@','#','']\n",
    "\n",
    "    wordsList = []\n",
    "    for tokenList in tokens:\n",
    "        words = []\n",
    "        for word in tokenList:\n",
    "            if word.lower() not in stopwordsList:\n",
    "                words.append(word.lower())\n",
    "        wordsList.append(words)\n",
    "\n",
    "    wnet = WordNetLemmatizer()\n",
    "    for i in range(len(wordsList)):\n",
    "        for j in range(len(wordsList[i])):\n",
    "            wordsList[i][j] = wnet.lemmatize(wordsList[i][j], pos='v')\n",
    "\n",
    "    for i in range(len(wordsList)):\n",
    "        wordsList[i] = ' '.join(wordsList[i])\n",
    "\n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Neeraj Kumar/nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Neeraj Kumar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Neeraj Kumar/nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Neeraj Kumar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\data\\SpamClassification.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wordsList \u001b[39m=\u001b[39m textProcessing(dataset)\n",
      "\u001b[1;32me:\\data\\SpamClassification.ipynb Cell 17\u001b[0m in \u001b[0;36mtextProcessing\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw]+\u001b[39m\u001b[39m'\u001b[39m,df[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][i])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         tokens\u001b[39m.\u001b[39mappend(msg)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     stopwordsList \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#     stopwordsList.extend([',','.','-','!',':','>','<','\\n','@','#','']\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/data/SpamClassification.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     wordsList \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Neeraj Kumar/nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Neeraj Kumar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "wordsList = textProcessing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'martin post tassos papadopoulos greek sculptor behind plan judge limestone mount kerdylio 70 miles east salonika far mount athos monastic community ideal patriotic sculpture well alexander granite feature 240 ft high 170 ft wide museum restore amphitheatre car park admire crowd plan mountain limestone granite limestone weather pretty fast yahoo group sponsor 4 dvds free p join http us click yahoo com pt6ybb nxieaa mg3haa 7gsolb tm unsubscribe group send email forteana unsubscribe egroups com use yahoo group subject http docs yahoo com info term'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = cv.fit_transform(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vect, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 58388)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 58388)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression()\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
